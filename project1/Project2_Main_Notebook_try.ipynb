{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import dlc_practical_prologue2 as prologue\n",
    "\n",
    "print('libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data for the first project, we are using one of the functions of *dlc_practical_prologe.py* that randomly generates one for us given a size parameter. It returns a tuple containing the: *training set, targets, classes* and *testing set, targets, classes*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "sets = prologue.generate_pair_sets(sample_size)\n",
    "\n",
    "train_set = sets[0]\n",
    "train_target = sets[1]\n",
    "train_classes = sets[2]\n",
    "\n",
    "test_set = sets[3]\n",
    "test_target = sets[4]\n",
    "test_classes = sets[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs are grayscale MNIST images consisting of two channels ($2 \\times 14 \\times 14$) representing two different digits. As visualized below, target vector contains the index of the channels which have the highest digit between the two (channel $0$ or $1$). \n",
    "\n",
    "Therefore, our task is to maximize the prediction of this boolean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the first training data point with two channels, target is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('For the first training data point with two channels, target is {}'.format(train_target[0]))\n",
    "fig1, axes = plt.subplots(1, 2, figsize = (10, 7))\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_title('Channel #{} - Class: {}'.format(i, train_classes[0][i]))\n",
    "    ax[i].imshow(train_set[0][i], cmap='copper')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model by moving it through the given mini batch size and using Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, eta, loss_criterion, optim, model, train_input, train_target, mini_batch_size):\n",
    "    optimizer = optim\n",
    "    for e in range(n_epochs):\n",
    "        acc_loss = 0 ## set error as 0 each iteration\n",
    "        ## Using mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size): \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            # print('Loss Shapes: Output: {}, Target: {}'.format(output.shape, train_target.narrow(0, b, mini_batch_size).shape))\n",
    "            loss = loss_criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad() ## setting the gradients to zero before the loss calculation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= eta * p.grad\n",
    "                    \n",
    "    # print('Final Output: ', output, output.shape)\n",
    "    # print(torch.argmax(output, dim=1))\n",
    "    print('Model Training Finished - Final loss after {} epochs: {}'.format(n_epochs, acc_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round the final values outputted from the network into binary values and compare them with the target binary matrix in order to get the number of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input, target):\n",
    "    with torch.no_grad():\n",
    "        rounded_to_binary = torch.round(input)\n",
    "        errors = torch.where(rounded_to_binary != train_target)\n",
    "    return len(errors[0]) / target.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt #1 - Binary Convolutional Network Classifier that takes the number of the hidden layers of the final fully connected layer as a parameter. \n",
    "\n",
    "As seen from the results below, it suffers from overfitting at the moment therefore it should be modified (dropout to be added next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCNN(nn.Module):\n",
    "    def __init__(self, hidden_layer_n, initial_layers=2, final_layers=2): ## defining the layers\n",
    "        super().__init__()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "                \n",
    "        # Feature Extractors\n",
    "        self.conv1 = nn.Conv2d(initial_layers, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Classifiers\n",
    "        self.linear1 = nn.Linear(256, hidden_layer_n)\n",
    "        self.linear2 = nn.Linear(hidden_layer_n, final_layers) ## output layers\n",
    "        \n",
    "        # batch norm\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        \n",
    "    ## Generally, strides for convolution layers are 1 and for maxpools are 2\n",
    "    def forward(self, x): \n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size = 2, stride = 2)\n",
    "        # print('First Conv Layer Shape', x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size = 2, stride = 2)\n",
    "        # print('Second Conv Layer Shape', x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size = 2, stride = 2)\n",
    "        x = self.flatten1(x)\n",
    "        # print('After Flattening', x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        # print('First Connected Layer', x.shape)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc2(F.relu(x)) ## added a relu before the final fully connected layer\n",
    "        x = self.flatten0(x)\n",
    "        # print('Final Output Shape {} \\n'.format(x.shape))\"\"\"\n",
    "        x = self.dropout1(self.batchnorm1(F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout1(self.batchnorm2(F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))))\n",
    "        x = self.dropout2(self.batchnorm3(F.relu(self.linear1(x.view(-1, 256)))))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([20])) must be the same as input size (torch.Size([20, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32000\\931811268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinaryCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32000\\3844442023.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs, eta, loss_criterion, optim, model, train_input, train_target, mini_batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;31m# print('Loss Shapes: Output: {}, Target: {}'.format(output.shape, train_target.narrow(0, b, mini_batch_size).shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0macc_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lower_python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lower_python\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    705\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                                                   reduction=self.reduction)\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lower_python\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([20])) must be the same as input size (torch.Size([20, 1]))"
     ]
    }
   ],
   "source": [
    "hidden_layers = [50, 200, 500, 1000]\n",
    "binary_criterion = nn.BCEWithLogitsLoss()\n",
    "eta = 0.001\n",
    "mini_batch_size = 20\n",
    "nb_epochs = 40\n",
    "n_runs = 1\n",
    "\n",
    "\n",
    "for i in range(n_runs):   \n",
    "    model = BinaryCNN(hidden_layer_n = 200)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.001)\n",
    "    train_model(nb_epochs, eta, binary_criterion, optimizer, model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "    \n",
    "    train_output = torch.sigmoid(model(train_set))\n",
    "    # print('test output', model(test_set))\n",
    "    test_output = torch.sigmoid(model(test_set))\n",
    "    \n",
    "    error_train = compute_nb_errors(model, train_output, train_target.to(torch.float32))\n",
    "    error_test = compute_nb_errors(model, test_output, test_target.to(torch.float32))\n",
    "    \n",
    "    print('Classification Error on the training set with {} hidden layers: {}%'.format(200, error_train * 100))\n",
    "    print('Classification Error on the testing set with {} hidden layers: {}%'.format(200, error_test * 100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6290ab70aa2c9e6859d722745d4fdeafb895ca1190e93c7ac9c8d926153eb965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
