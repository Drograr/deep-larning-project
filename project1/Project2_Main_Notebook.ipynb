{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import dlc_practical_prologue2 as prologue\n",
    "\n",
    "print('libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data for the first project, we are using one of the functions of *dlc_practical_prologe.py* that randomly generates one for us given a size parameter. It returns a tuple containing the: *training set, targets, classes* and *testing set, targets, classes*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "sets = prologue.generate_pair_sets(sample_size)\n",
    "\n",
    "train_set = sets[0]\n",
    "train_target = sets[1]\n",
    "train_classes = sets[2]\n",
    "\n",
    "test_set = sets[3]\n",
    "test_target = sets[4]\n",
    "test_classes = sets[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs are grayscale MNIST images consisting of two channels ($2 \\times 14 \\times 14$) representing two different digits. As visualized below, target vector contains the index of the channels which have the highest digit between the two (channel $0$ or $1$). \n",
    "\n",
    "Therefore, our task is to maximize the prediction of this boolean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the first training data point with two channels, target is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('For the first training data point with two channels, target is {}'.format(train_target[0]))\n",
    "fig1, axes = plt.subplots(1, 2, figsize = (10, 7))\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_title('Channel #{} - Class: {}'.format(i, train_classes[0][i]))\n",
    "    ax[i].imshow(train_set[0][i], cmap='copper')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model by moving it through the given mini batch size and using Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, eta, loss_criterion, model, train_input, train_target, train_classes, mini_batch_size):\n",
    "    aux_loss_criterion = nn.CrossEntropyLoss()\n",
    "    gamma = 1.0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.01, lr = eta)\n",
    "    for _ in range(n_epochs):\n",
    "        acc_loss = 0 ## set error as 0 each iteration\n",
    "        ## Using mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size): \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            # output, output_aux = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            # print('output shape', output.shape, output_aux.shape)\n",
    "            # print(output)\n",
    "            # print(output_aux)\n",
    "            # print('Loss Shapes: Output: {}, Target: {}'.format(output.shape, train_target.narrow(0, b, mini_batch_size).shape))\n",
    "            loss1 = loss_criterion(output, train_target.narrow(0, b, mini_batch_size).float())\n",
    "            # print(output_aux.shape, train_classes.narrow(0, b, mini_batch_size).shape)\n",
    "            # print(train_classes.narrow(0, b, mini_batch_size))\n",
    "            # aux_loss = aux_loss_criterion(output_aux, train_classes.narrow(0, b, mini_batch_size))\n",
    "            # loss = loss1 + (gamma * aux_loss)\n",
    "            \n",
    "            loss = loss1\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "\n",
    "            model.zero_grad() ## setting the gradients to zero before the loss calculation\n",
    "            loss.backward()\n",
    "            optimizer.step()                    \n",
    "    # print('Final Output: ', output, output.shape)\n",
    "    # print(torch.argmax(output, dim=1))\n",
    "    # print('Model Training Finished - Final loss after {} epochs: {}'.format(n_epochs, acc_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round the final values outputted from the network into binary values and compare them with the target binary matrix in order to get the number of errors.\n",
    "\n",
    "The sigmoid function $\\sigma(x) = \\frac 1 {1+exp(-x)}$ is applied to the output to distribute the output values between $0$ and $1$, after which a rounding process happens to finally compare them with our binary targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input, target, mini_batch_size):\n",
    "    nb_errors = 0\n",
    "    with torch.no_grad():\n",
    "        for b in range(0, input.size(0), mini_batch_size):\n",
    "            output = torch.sigmoid(model(input.narrow(0, b, mini_batch_size)))\n",
    "            errors = torch.where(torch.round(output) != target.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            ## Errors return a tuple with the first element being a tensor with indexes\n",
    "            ## Where the targets and predictions dont match\n",
    "            ## Therefore we get the size of this tensor as a number of errors\n",
    "            nb_errors += errors[0].size(0) \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining both models\n",
    "\n",
    "The first model will be a classical Binary Convolutional Network Classifier inspired by LeNet, that takes the number of the hidden layers of the final fully connected layer as a parameter. \n",
    "As seen from the results below, it suffers from overfitting at the moment therefore it should be modified (dropout to be added next).\n",
    "\n",
    "The second model will be a Multi-Layer Perceptron with only linear hidden layers, constructed in order to compare the difference between this and the convolutional network, measuring the overfitting too. \n",
    "\n",
    "They both includes multiple dropout layers with respective rates of $0.4$ and share a weight decay parameter on the Adam Optimizer, as measures to combat overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see file for code\n",
    "from MLP_model import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see file for code\n",
    "from CNN_model import BinaryCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training cell for one model, to be easily switched between MLP and CNN.\n",
    "\n",
    "Hyperparameters after benchmarking (average over 10 runs): $40$ - $50$ batch size seems to be the best, $0.001$ or $0.0001$ weight decay, $10-25$ epochs sufficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing for auxilary loss\n",
    "\n",
    "# target = train_classes.narrow(0, 1, 5)\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# # target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# # output = loss(input, target)\n",
    "# # output\n",
    "\n",
    "# input = torch.randn(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_criterion = nn.BCEWithLogitsLoss()\n",
    "eta = 0.001\n",
    "mini_batch_size = 50\n",
    "nb_epochs = 25\n",
    "n_runs = 5\n",
    "hidden_layers = [4096, 2048, 1024, 512, 256, 128, 64, 32]\n",
    "epochs = [1, 5, 10, 15, 25, 40, 50, 80, 100, 200, 1000]\n",
    "batches = [40, 50, 100, 500]\n",
    "dropouts = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "\n",
    "# for d in dropouts:\n",
    "avg_error_train = []\n",
    "avg_error_test = []\n",
    "for i in range(n_runs):\n",
    "    model = BinaryCNN(dropout_rate = 0.2)\n",
    "    train_model(nb_epochs, eta, binary_criterion, model, train_set, train_target, train_classes.to(torch.long), mini_batch_size)\n",
    "    \n",
    "    model.eval()    \n",
    "    error_train = compute_nb_errors(model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "    avg_error_train.append(error_train / train_set.size(0))\n",
    "    error_test = compute_nb_errors(model, test_set, test_target.to(torch.float32), mini_batch_size)\n",
    "    avg_error_test.append(error_test / train_set.size(0))\n",
    "\n",
    "print('Classification Error on the training set with {} fc dropout - Average: {}%'.format('0.2', (sum(avg_error_train) / n_runs) * 100))\n",
    "print('Classification Error on the testing set with {} fc dropout - Average: {}%, Minimum: {}%'.format('0.2', (sum(avg_error_test) / n_runs) * 100, min(avg_error_test) * 100))           \n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training cell for both models to calculate their performance on the testing set and to be used for benchmarking & fine-tuning with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shared parameters to both models\n",
    "hidden_layers = [50, 200, 500, 1000]\n",
    "binary_criterion = nn.BCEWithLogitsLoss()\n",
    "mini_batch_size = 50\n",
    "nb_epochs = 25\n",
    "n_runs = 10\n",
    "eta = 0.001\n",
    "\n",
    "# Lists for Plotting & Benchmarks Purposes\n",
    "all_epochs = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100]\n",
    "all_errors_cnn_train = []\n",
    "all_errors_mlp_train = []\n",
    "all_errors_cnn = []\n",
    "all_errors_mlp = []\n",
    "\n",
    "\n",
    "for e in range(len(all_epochs)):\n",
    "    avg_error_train_cnn = []\n",
    "    avg_error_test_cnn = []\n",
    "    avg_error_train_mlp = []\n",
    "    avg_error_test_mlp = []\n",
    "    print('Average over {} Epochs'.format(all_epochs[e]))\n",
    "    for i in range(n_runs):   \n",
    "        conv_model = BinaryCNN(dropout_rate = 0.2)\n",
    "        mlp_model = MLP(dropout_rate = 0.4)\n",
    "        \n",
    "        ## Training CNN\n",
    "        train_model(all_epochs[e], eta, binary_criterion, conv_model, train_set, train_target.to(torch.float32), train_classes, mini_batch_size)\n",
    "        ## Training MLP\n",
    "        train_model(all_epochs[e], eta, binary_criterion, mlp_model, train_set, train_target.to(torch.float32), train_classes, mini_batch_size)\n",
    "            \n",
    "        conv_model.eval()\n",
    "        conv_errors_train = compute_nb_errors(conv_model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "        conv_errors_test = compute_nb_errors(conv_model, test_set, test_target.to(torch.float32), mini_batch_size)\n",
    "        avg_error_train_cnn.append(conv_errors_train / train_set.size(0))\n",
    "        avg_error_test_cnn.append(conv_errors_test / test_set.size(0))\n",
    "        \n",
    "        mlp_model.eval()\n",
    "        mlp_errors_train = compute_nb_errors(mlp_model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "        mlp_errors_test = compute_nb_errors(mlp_model, test_set, test_target.to(torch.float32), mini_batch_size)\n",
    "        avg_error_train_mlp.append(mlp_errors_train / train_set.size(0))\n",
    "        avg_error_test_mlp.append(mlp_errors_test / test_set.size(0))\n",
    "\n",
    "    all_errors_cnn_train.append(sum(avg_error_train_cnn) / n_runs)\n",
    "    all_errors_cnn.append(sum(avg_error_test_cnn) / n_runs)\n",
    "    \n",
    "    all_errors_mlp_train.append(sum(avg_error_train_mlp) / n_runs)\n",
    "    all_errors_mlp.append(sum(avg_error_test_mlp) / n_runs)\n",
    "            \n",
    "    print('CNN - Classification Error on the training set: Average: {:.2f}%, Minimum: {:.2f}%'.format((all_errors_cnn_train[e] * 100), (min(avg_error_train_cnn) * 100)))\n",
    "    print('CNN - Classification Error on the testing set: Average: {:.2f}%, Minimum: {:.2f}%'.format((all_errors_cnn[e] * 100), (min(avg_error_test_cnn) * 100)))\n",
    "    print('MLP - Classification Error on the training set: Average: {:.2f}%, Minimum: {:.2f}%'.format((all_errors_mlp_train[e] * 100), (min(avg_error_train_mlp) * 100)))\n",
    "    print('MLP - Classification Error on the testing set: Average: {:.2f}%, Minimum: {:.2f}%\\n'.format((all_errors_mlp[e] * 100), (min(avg_error_test_mlp) * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can view from the plot below, the overfitting on the testing set persists no matter the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Error rate on both models - Average over 10runs - Dropout')\n",
    "axs[0].set_title(\"Train Set\")\n",
    "# plt.xlabel('Number of Epochs')\n",
    "# plt.ylabel('Error Rate')\n",
    "axs[0].plot(all_epochs, all_errors_cnn_train, alpha=0.6, linewidth=4, color='turquoise', label='CNN')\n",
    "axs[0].plot(all_epochs, all_errors_mlp_train, alpha=0.6, linewidth=4, color='maroon', label='MLP')\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "axs[1].set\n",
    "axs[1].set_title(\"Test Set\")\n",
    "axs[1].plot(all_epochs, all_errors_cnn, alpha=0.6, linewidth=4, color='turquoise', label='CNN')\n",
    "axs[1].plot(all_epochs, all_errors_mlp, alpha=0.6, linewidth=4, color='maroon', label='MLP')\n",
    "axs[1].legend(loc='upper right')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Number of Epochs', ylabel='Error Rate', ylim=(-0.01, 0.5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6290ab70aa2c9e6859d722745d4fdeafb895ca1190e93c7ac9c8d926153eb965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
