{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import dlc_practical_prologue2 as prologue\n",
    "\n",
    "print('libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data for the first project, we are using one of the functions of *dlc_practical_prologe.py* that randomly generates one for us given a size parameter. It returns a tuple containing the: *training set, targets, classes* and *testing set, targets, classes*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "sets = prologue.generate_pair_sets(sample_size)\n",
    "\n",
    "train_set = sets[0]\n",
    "train_target = sets[1]\n",
    "train_classes = sets[2]\n",
    "\n",
    "test_set = sets[3]\n",
    "test_target = sets[4]\n",
    "test_classes = sets[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs are grayscale MNIST images consisting of two channels ($2 \\times 14 \\times 14$) representing two different digits. As visualized below, target vector contains the index of the channels which have the highest digit between the two (channel $0$ or $1$). \n",
    "\n",
    "Therefore, our task is to maximize the prediction of this boolean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the first training data point with two channels, target is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('For the first training data point with two channels, target is {}'.format(train_target[0]))\n",
    "fig1, axes = plt.subplots(1, 2, figsize = (10, 7))\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_title('Channel #{} - Class: {}'.format(i, train_classes[0][i]))\n",
    "    ax[i].imshow(train_set[0][i], cmap='copper')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model by moving it through the given mini batch size and using Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, eta, loss_criterion, optim, model, train_input, train_target, mini_batch_size):\n",
    "    optimizer = optim\n",
    "    for e in range(n_epochs):\n",
    "        acc_loss = 0 ## set error as 0 each iteration\n",
    "        ## Using mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size): \n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            # print('Loss Shapes: Output: {}, Target: {}'.format(output.shape, train_target.narrow(0, b, mini_batch_size).shape))\n",
    "            loss = loss_criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad() ## setting the gradients to zero before the loss calculation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= eta * p.grad\n",
    "                    \n",
    "    # print('Final Output: ', output, output.shape)\n",
    "    # print(torch.argmax(output, dim=1))\n",
    "    print('Model Training Finished - Final loss after {} epochs: {}'.format(n_epochs, acc_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round the final values outputted from the network into binary values and compare them with the target binary matrix in order to get the number of errors.\n",
    "\n",
    "The sigmoid function $\\sigma(x) = \\frac 1 {1+exp(-x)}$ is applied to the output to distribute the output values between $0$ and $1$, after which a rounding process happens to finally compare them with our binary targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input, target, mini_batch_size):\n",
    "    nb_errors = 0\n",
    "    with torch.no_grad():\n",
    "        for b in range(0, input.size(0), mini_batch_size):\n",
    "            output = torch.sigmoid(model(input.narrow(0, b, mini_batch_size)))\n",
    "            errors = torch.where(torch.round(output) != target.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            ## Errors return a tuple with the first element being a tensor with indexes\n",
    "            ## Where the targets and predictions dont match\n",
    "            ## Therefore we get the size of this tensor as a number of errors\n",
    "            nb_errors += errors[0].size(0) \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt #1 - Binary Convolutional Network Classifier that takes the number of the hidden layers of the final fully connected layer as a parameter. \n",
    "\n",
    "As seen from the results below, it suffers from overfitting at the moment therefore it should be modified (dropout to be added next)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCNN(nn.Module):\n",
    "    def __init__(self, hidden_layer_n, dropout_rate1, dropout_rate2): ## defining the layers\n",
    "        super().__init__()\n",
    "        self.debug_mode = True\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate1)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate2)\n",
    "        \n",
    "        self.flatten0 = nn.Flatten(0)\n",
    "        self.flatten1 = nn.Flatten(1)\n",
    "        \n",
    "        # Feature Extractors\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Data Normalizers\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Classifiers\n",
    "        self.fc1 = nn.Linear(128, hidden_layer_n)\n",
    "        self.fc2 = nn.Linear(hidden_layer_n, hidden_layer_n * 2)\n",
    "        self.fc3 = nn.Linear(hidden_layer_n * 2, 1) ## output layers\n",
    "        \n",
    "    ## Generally, strides for convolution layers are 1 and for maxpools are 2\n",
    "    def forward(self, x): \n",
    "        x = self.batchnorm1(F.max_pool2d(F.relu(self.dropout1(self.conv1(x))), kernel_size = 2, stride = 2))\n",
    "        x = self.batchnorm2(F.max_pool2d(F.relu(self.dropout1(self.conv2(x))), kernel_size = 2, stride = 2))\n",
    "        x = self.fc1(self.dropout2(self.flatten1(x)))\n",
    "        x = self.fc3(F.relu(self.fc2(x))) ## added a relu before the final fully connected layer\n",
    "        x = self.flatten0(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #0 - Training Started\n",
      "Model Training Finished - Final loss after 50 epochs: 28.48733899812214\n",
      "Classification Error on the training set with 200 hidden layers: 4.3999999999999995%\n",
      "Classification Error on the testing set with 200 hidden layers: 17.299999999999997%\n",
      "\n",
      "Run #1 - Training Started\n",
      "Model Training Finished - Final loss after 50 epochs: 24.287112187535968\n",
      "Classification Error on the training set with 200 hidden layers: 4.3999999999999995%\n",
      "Classification Error on the testing set with 200 hidden layers: 18.7%\n",
      "\n",
      "Run #2 - Training Started\n",
      "Model Training Finished - Final loss after 50 epochs: 27.662959099252475\n",
      "Classification Error on the training set with 200 hidden layers: 9.1%\n",
      "Classification Error on the testing set with 200 hidden layers: 21.8%\n",
      "\n",
      "Run #3 - Training Started\n",
      "Model Training Finished - Final loss after 50 epochs: 31.212552471202798\n",
      "Classification Error on the training set with 200 hidden layers: 9.700000000000001%\n",
      "Classification Error on the testing set with 200 hidden layers: 21.5%\n",
      "\n",
      "Run #4 - Training Started\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [50, 200, 500, 1000]\n",
    "binary_criterion = nn.BCEWithLogitsLoss()\n",
    "eta = 0.001\n",
    "mini_batch_size = 5\n",
    "nb_epochs = 50\n",
    "n_runs = 5\n",
    "\n",
    "\n",
    "for i in range(n_runs):   \n",
    "    model = BinaryCNN(hidden_layer_n = hidden_layers[1], dropout_rate1= 0.2,dropout_rate2= 0.4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.001)\n",
    "    print('Run #{} - Training Started'.format(i))\n",
    "    train_model(nb_epochs, eta, binary_criterion, optimizer, model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "        \n",
    "    model.eval()\n",
    "    errors_train = compute_nb_errors(model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "    errors_test = compute_nb_errors(model, test_set, test_target.to(torch.float32), mini_batch_size)\n",
    "    \n",
    "    print('Classification Error on the training set with {} hidden layers: {}%'.format(hidden_layers[1], (errors_train / train_set.size(0)) * 100))\n",
    "    print('Classification Error on the testing set with {} hidden layers: {}%'.format(hidden_layers[1], (errors_test / test_set.size(0))  * 100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt #2 - Multi-Layer Perceptron with only linear hidden layers, constructed in order to compare the difference between this and the convolutional network, measuring the overfitting too. This network includes multiple dropout layers with respective rates of $0.4$ and a weight decay parameter on the Adam Optimizer, as measures to combat the above mentioned overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dropout_rate): ## defining the layers\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
    "        self.flatten_dim_0 = nn.Flatten(0)\n",
    "        self.flatten_dim_1 = nn.Flatten(1)\n",
    "\n",
    "        # Fully Connected Layers        \n",
    "        self.fc1 = nn.Linear(2 * 14 * 14, 1028) ## (2x14x14) images\n",
    "        self.fc2 = nn.Linear(1028, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1) ## output layers\n",
    "\n",
    "        \n",
    "    ## Generally, strides for convolution layers are 1 and for maxpools are 2\n",
    "    def forward(self, x): \n",
    "        x = self.flatten_dim_1(x)\n",
    "        # print('Flattened Input Shape: ', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print('First FC Layer Shape', x.shape)\n",
    "        x = F.relu(self.dropout(self.fc2(x)))\n",
    "        x = self.batchnorm1(x)\n",
    "        # print('Second FC Layer Shape', x.shape)\n",
    "        x = F.relu(self.dropout(self.fc3(x)))\n",
    "        x = self.batchnorm2(x)\n",
    "        # print('Third FC Layer Shape', x.shape)\n",
    "        x = self.fc4(x)\n",
    "        x = self.flatten_dim_0(x)\n",
    "        # print('Final Output Shape {} \\n'.format(x.shape))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this network, the batch size was increased to 200 as it improved the accuracy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Finished - Final loss after 50 epochs: 14.873949751257896\n",
      "Classification Error on the training set: 2.5%\n",
      "Classification Error on the testing set: 18.2%\n",
      "\n",
      "Model Training Finished - Final loss after 50 epochs: 12.84285993874073\n",
      "Classification Error on the training set: 2.5%\n",
      "Classification Error on the testing set: 18.2%\n",
      "\n",
      "Model Training Finished - Final loss after 50 epochs: 12.835561633110046\n",
      "Classification Error on the training set: 2.5%\n",
      "Classification Error on the testing set: 18.2%\n",
      "\n",
      "Model Training Finished - Final loss after 50 epochs: 15.405562490224838\n",
      "Classification Error on the training set: 2.5%\n",
      "Classification Error on the testing set: 18.2%\n",
      "\n",
      "Model Training Finished - Final loss after 50 epochs: 13.629915490746498\n",
      "Classification Error on the training set: 2.5%\n",
      "Classification Error on the testing set: 18.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_criterion = nn.BCEWithLogitsLoss()\n",
    "eta = 1e-07\n",
    "mini_batch_size = 20\n",
    "nb_epochs = 50\n",
    "n_runs = 5\n",
    "\n",
    "# For Plotting Purposes\n",
    "all_epochs = []\n",
    "all_training_errors = []\n",
    "all_testing_errors = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # all_epochs.append(i)\n",
    "    model = MLP(dropout_rate = 0.4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.1)\n",
    "    train_model(nb_epochs, eta, binary_criterion, optimizer, model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "        \n",
    "    error_train = compute_nb_errors(model, train_set, train_target.to(torch.float32), mini_batch_size)\n",
    "    error_test = compute_nb_errors(model, test_set, test_target.to(torch.float32), mini_batch_size)\n",
    "    \n",
    "    # all_training_errors.append(error_train)\n",
    "    # all_testing_errors.append(error_test)\n",
    "        \n",
    "    print('Classification Error on the training set: {}%'.format((errors_train / train_set.size(0)) * 100))\n",
    "    print('Classification Error on the testing set: {}%'.format((errors_test / train_set.size(0)) * 100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can view from the plot below, the overfitting on the testing set persists no matter the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.title(\"Error rate with regard to number of epochs - Multi Layer Perceptron\")\n",
    "# plt.xlabel('Number of Epochs')\n",
    "# plt.ylabel('Error Rate')\n",
    "# plt.plot(all_epochs, all_training_errors, alpha=0.6, linewidth=4, color='turquoise', label='Train Error')\n",
    "# plt.plot(all_epochs, all_testing_errors, alpha=0.6, linewidth=4, color='maroon', label = 'Testing Error')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6290ab70aa2c9e6859d722745d4fdeafb895ca1190e93c7ac9c8d926153eb965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
